{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0af4dd4-b531-4109-a452-4a59a57a5f34",
   "metadata": {},
   "source": [
    "## CELL for Natural Language Generation\n",
    "\n",
    "This notebook illustrates how to use the `CELL` algorithms for generating contrastive explanations. Two different algorithms are demonstrated: `CELL` (an intelligent search algorithm that is subject to a budget on model calls) and `mCELL` (a myopic algorithm that is more expensive when explaining the responses of longer prompts). \n",
    "<br><br>\n",
    "The first set of examples demonstrate CELL and mCELL when prompting a relatively small LLM (`flan-t5-large`). This is followed by and example demonstrating CELL on a larger instruction-based LLM (`granite-3.3-8B-instruct`), which also demonstrates how a user can incorporate an LLM's chat template. These examples all use a wrapper class for Huggingface models called `HFModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d1653-2b0d-4890-a2d8-bf3f12750fdb",
   "metadata": {},
   "source": [
    "### Import Standard Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148d17b-88dc-477a-82f6-996a4bc7e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a77a5-486e-420b-85b5-7cb6e07ef1cd",
   "metadata": {},
   "source": [
    "### Import icx classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6542ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icx360.algorithms.cell.CELL import CELL # this imports a budgeted version of CELL\n",
    "from icx360.utils.model_wrappers import HFModel\n",
    "from icx360.utils.general_utils import select_device, fix_seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff02d1f-7ac0-4875-936c-67c27dbd9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed for experimentation\n",
    "seed = 12345\n",
    "fix_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6cb4f9-68f1-4619-ba03-3ea46b71fe8c",
   "metadata": {},
   "source": [
    "### Load model, use icx wrapper class, and create explainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8633f6e-321c-4ce8-b3b7-054cecc83c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Note device is set automatically according to your system. You can overwite device here if you choose.\n",
    "device = select_device()\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_expl = HFModel(model, tokenizer) # icx wrapped model\n",
    "num_return_sequences = 10 # number of sequences returned when doing generation for mask infilling\n",
    "infiller = 't5' # function used to input text with a mask token and output text with mask replaced by text ('t5' and 'bart')\n",
    "scalarizer = 'preference' # scalarizer to use to determine if a contrast is found (must be from ['preference', 'nli', 'contradiction', 'bleu']\n",
    "# if no device is passed to CELL, it will be set automatically according to your system\n",
    "explainer = CELL(model_expl, num_return_sequences=num_return_sequences, infiller=infiller, scalarizer=scalarizer, device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b00ed-8e4d-4e83-b781-0899f1c15bac",
   "metadata": {},
   "source": [
    "### Fix parameters for budgeted CELL explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c308433-989e-4dd3-8f1d-e1a5a91a9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_k = 2 \n",
    "epsilon_contrastive = 0.25 # amount of change in response to deem a contrastive explanation\n",
    "radius = 3 # radius for sampling near a previously modified token \n",
    "budget = 50 # maximum number of queries allowed from infilling model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377a617-492d-45de-8a4d-ce9e8071a320",
   "metadata": {},
   "source": [
    "### Feed an input prompt to the explainer and generate contrastive explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b126c490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Contrastive Explanations for Large Language Models\n",
      "Running outer iteration 1\n",
      "Stopping because contrastive threshold has been passed\n",
      "10 model calls made.\n",
      "Contrastive Explanation Solution\n",
      "Scalarizer: preference\n",
      "Input prompt: What are the most popular activities for children for elementary school age?\n",
      "Input response: play dough\n",
      "Contrastive prompt: are the most important foods for children for elementary school age?\n",
      "Contrastive response: a balanced diet\n",
      "Modifications made: \n",
      "        popular activities->important foods\n",
      "        What are->are\n",
      "Preference decreased.\n"
     ]
    }
   ],
   "source": [
    "input_text =\"What are the most popular activities for children for elementary school age?\"\n",
    "result = explainer.explain_instance(input_text, radius=radius, budget=budget, split_k=split_k, epsilon_contrastive=epsilon_contrastive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45daf69-aaba-4ffd-9122-cc578229874a",
   "metadata": {},
   "source": [
    "**Input prompt** is the user prompt for which one wants to explain the response of the LLM.<br>\n",
    "**Input response** is the response of the LLM to the input prompt.<br>\n",
    "**Contrastive prompt** is the new prompt after masking and infilling certain words.<br>\n",
    "**Contrastive response** is the response of the LLM to the contrastive prompt.\n",
    "<br><br>\n",
    "The above example shows that if a user's inquiry is instead the contrastive prompt (obtained by making the modifications to the input prompt), the new response response, termed the contrastive response, would be given. The preferability of the contrastive response over the original input response is given (in regards to the original input prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1c944-347c-40a2-8b71-0990bf7fdb82",
   "metadata": {},
   "source": [
    "### Example using myopic CELL (mCELL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f819182-c6b0-4593-9528-9a015d70a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting (myopic) Contrastive Explanations for Large Language Models\n",
      "Running iteration 1\n",
      "Stopping because contrastive threshold has been passed\n",
      "6 model calls made.\n",
      "Contrastive Explanation Solution\n",
      "Scalarizer: preference\n",
      "Input prompt: What are the most popular activities for children for elementary school age?\n",
      "Input response: play dough\n",
      "Contrastive prompt: What are the most popular activities online for elementary school age?\n",
      "Contrastive response: wikihow\n",
      "Modifications made: \n",
      "        for children->online\n",
      "Preference decreased.\n"
     ]
    }
   ],
   "source": [
    "from icx360.algorithms.cell.mCELL import mCELL # this imports a myopic version of CELL\n",
    "# if no device is passed to mCELL, it will be set automatically according to your system\n",
    "explainer = mCELL(model_expl, num_return_sequences=num_return_sequences, infiller=infiller, scalarizer=scalarizer, device=device)\n",
    "\n",
    "fix_seed(seed)\n",
    "input_text =\"What are the most popular activities for children for elementary school age?\"\n",
    "result = explainer.explain_instance(input_text, split_k=split_k, epsilon_contrastive=epsilon_contrastive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a97efb-77e7-4bab-8ce6-773e948634c3",
   "metadata": {},
   "source": [
    "The above example shows how to use the myopic CELL algorithm which may give different explanations due to the different explanation search used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6031caa-a7d0-46aa-8390-74daa9f85fbc",
   "metadata": {},
   "source": [
    "### Example using instruction fine-tuned LLM with a chat template (granite-3.3-8b-instruct)\n",
    "\n",
    "Import standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a369a336-8e6a-4ca2-a384-0ca2ef8714c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 4/4 [01:20<00:00, 20.18s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [01:02<00:00, 31.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_expl = HFModel(model, tokenizer) # icx wrapped model\n",
    "\n",
    "num_return_sequences = 10 # number of sequences returned when doing generation for mask infilling\n",
    "infiller = 't5' # function used to input text with a mask token and output text with mask replaced by text ('t5' and 'bart')\n",
    "scalarizer = 'preference' # scalarizer to use to determine if a contrast is found (must be from ['preference', 'nli', 'contradiction', 'bleu']\n",
    "explainer = CELL(model_expl, num_return_sequences=num_return_sequences, infiller=infiller, scalarizer=scalarizer, scalarizer_model_path='stanfordnlp/SteamSHP-flan-t5-xl', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51081a8f-1bb8-4d3c-a411-199c4b723d6f",
   "metadata": {},
   "source": [
    "The difference now is that we must pass additional parameters to the explainer that are required for using an instruction fine-tuned model. Also note that this example shows how a user can select which scalarizer model to use. The preference scalarizer is default to use `stanfordnlp/SteamSHP-flan-t5-large` which is 4 times smaller than the xl model, but a user can override the default as done above by passing the `scalarizer_model_path` parameter. In the `model_params` object below,\n",
    "<br><br>\n",
    "**chat_template** is an indicator that the user wants to use the known chat template of the LLM<br>\n",
    "**system_prompt** is the instruction to be followed by the LLM<br>\n",
    "**pad_token_id** is an LLM-specific parameter for padding purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97de3b6f-f88e-4079-9bad-f9ffcef3590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "model_params[\"chat_template\"] = True\n",
    "model_params[\"system_prompt\"] = \"Please respond to the following statement or question very briefly in less than 10 words.\" \n",
    "model_params[\"pad_token_id\"] = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac5587-70fd-45ee-8232-734faf787185",
   "metadata": {},
   "source": [
    "### Fix parameters for budgeted CELL explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "249a1e0a-4a44-4cb9-8e61-e1a043c1f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_k = 2 \n",
    "epsilon_contrastive = 0.25 # amount of change in response to deem a contrastive explanation\n",
    "radius = 3 # radius for sampling near a previously modified token \n",
    "budget = 20 # maximum number of queries allowed from infilling model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560896c8-8305-49ed-ae85-f5db891baa44",
   "metadata": {},
   "source": [
    "### Feed an input prompt to the explainer and generate contrastive explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f04ad0-d24c-4cd3-9e5c-616034f0029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Contrastive Explanations for Large Language Models\n",
      "Running outer iteration 1\n",
      "Stopping because contrastive threshold has been passed\n",
      "6 model calls made.\n",
      "Contrastive Explanation Solution\n",
      "Scalarizer: preference\n",
      "Input prompt: What are the most popular activities for children for elementary school age?\n",
      "Input response: Playing, reading, drawing, sports, and educational games.\n",
      "Contrastive prompt: are the most popular books for children for elementary school age?\n",
      "Contrastive response: \"Harry Potter,\" \"Captain Underpants,\" \"Diary of a Wimp\n",
      "Modifications made: \n",
      "        What are->are\n",
      "        popular activities->popular books\n",
      "Preference increased.\n"
     ]
    }
   ],
   "source": [
    "fix_seed(seed)\n",
    "input_text =\"What are the most popular activities for children for elementary school age?\"\n",
    "result = explainer.explain_instance(input_text, radius=radius, budget=budget, split_k=split_k, epsilon_contrastive=epsilon_contrastive, model_params=model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09c6eb-5d44-4582-8819-2e20f1fb977e",
   "metadata": {},
   "source": [
    "Note that both the input and contastive responses are much more detailed here due to the different LLM being prompted. As with the above examples, the explanation modifies the prompt to make a different inquiry resulting in a contrastive response with a change in preferability over the initial input response. Note that the model being explained is now a significantly better model than the `google/flan-t5-large` used in the other examples above and is thus more likely to give appropriate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ed59e-5129-4aa2-98f5-128c296efdbc",
   "metadata": {},
   "source": [
    "### Note on using VLLM\n",
    "\n",
    "In order to use a model through VLLM, a user would create a model object (e.g., using OpenAI API) and wrap it with the `VLLMModel` wrapper (`from icx360.utils.model_wrappers import VLLMModel`) rather then the `HFModel` wrapper used above. Explanations can be created by passing this `VLLMModel` object in place of the `HFModel` object to the `CELL` (or `mCELL`) objects as in the above example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
